{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elmahmudi/OrganImageSegmentation/blob/main/COS6030_B_Coursework_Starter_Vision%2BPrompting_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P20TrNfXxjRV"
      },
      "source": [
        "# COS6030-B Coursework Starter — Computer Vision **with Prompting** (Transformers/CLIP)\n",
        "**Runs on Colab CPU** (small subsets).\n",
        "\n",
        "**You will:**\n",
        "- Zero-shot CLIP classification with prompts\n",
        "- Prompt engineering & evaluation\n",
        "- Few-shot linear probe (CPU-friendly)\n",
        "- Text→Image retrieval with CLIP\n",
        "- Directions to extend into coursework\n",
        "- Add GitHub\n",
        "dd"
      ],
      "id": "P20TrNfXxjRV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8YGdoWxjRY"
      },
      "source": [
        "> **Instructor notes:** Relates Mix+Squash & transformers to practical prompting. CPU-only by using frozen CLIP + linear probe."
      ],
      "id": "bI8YGdoWxjRY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMhQ2IRJxjRY"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Setup (CPU) — installs\n",
        "!pip -q install transformers>=4.40 torchvision>=0.16 scikit-learn matplotlib pillow --progress-bar off"
      ],
      "id": "FMhQ2IRJxjRY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ECUNQ-RxjRa",
        "outputId": "f2ebd800-878f-4f83-b64a-9d3b28d47ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.9.0+cu126 | Device: cpu\n"
          ]
        }
      ],
      "source": [
        "#@title Imports & environment check (CPU)\n",
        "import os, random, numpy as np, matplotlib.pyplot as plt, torch\n",
        "from torchvision import datasets\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
        "device = torch.device(\"cpu\"); print(\"Torch:\", torch.__version__, \"| Device:\", device)"
      ],
      "id": "9ECUNQ-RxjRa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBw9U7v3xjRa"
      },
      "source": [
        "## 1) Problem framing\n",
        "Small CIFAR-10 subset → zero-shot CLIP with prompts → prompt engineering → few-shot linear probe → retrieval."
      ],
      "id": "vBw9U7v3xjRa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYRnKYvVxjRb"
      },
      "source": [
        "## 2) Data — CIFAR-10 (5 classes for speed)\n",
        "Classes: `airplane, automobile, bird, cat, dog`."
      ],
      "id": "yYRnKYvVxjRb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIBhJVcWxjRb",
        "outputId": "f7552e08-8146-4253-b760-2f0446769ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 27.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 750 | Test 500 | Classes: ['airplane', 'automobile', 'bird', 'cat', 'dog']\n"
          ]
        }
      ],
      "source": [
        "#@title Load subset\n",
        "wanted_classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"dog\"]\n",
        "cifar10_labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "wanted_idx = [cifar10_labels.index(c) for c in wanted_classes]\n",
        "train_all = datasets.CIFAR10(\"./data\", train=True, download=True)\n",
        "test_all  = datasets.CIFAR10(\"./data\", train=False, download=True)\n",
        "\n",
        "def filter_subset(dataset, wanted_idx, max_per_class=200):\n",
        "    counts = {i:0 for i in wanted_idx}; images, labels = [], []\n",
        "    for img, y in dataset:\n",
        "        if y in wanted_idx and counts[y] < max_per_class:\n",
        "            images.append(img); labels.append(wanted_idx.index(y)); counts[y]+=1\n",
        "        if all(counts[i] >= max_per_class for i in wanted_idx): break\n",
        "    return images, np.array(labels)\n",
        "\n",
        "X_train_imgs, y_train = filter_subset(train_all, wanted_idx, 150)  # 750 imgs\n",
        "X_test_imgs,  y_test  = filter_subset(test_all,  wanted_idx, 100)  # 500 imgs\n",
        "print(f\"Train {len(X_train_imgs)} | Test {len(X_test_imgs)} | Classes: {wanted_classes}\")"
      ],
      "id": "aIBhJVcWxjRb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvyMovtzxjRc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Visual check\n",
        "rows, cols = 2, 5\n",
        "import matplotlib.pyplot as plt, random\n",
        "plt.figure(figsize=(10,4))\n",
        "for i in range(rows*cols):\n",
        "    idx = random.randrange(len(X_train_imgs))\n",
        "    plt.subplot(rows, cols, i+1); plt.imshow(X_train_imgs[idx]); plt.axis('off')\n",
        "plt.suptitle(\"Random training images\"); plt.show()"
      ],
      "id": "VvyMovtzxjRc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILm5xELpxjRd"
      },
      "source": [
        "## 3) Zero-shot CLIP (prompting)\n",
        "Encode prompts for each class; choose the class with highest cosine similarity to the image."
      ],
      "id": "ILm5xELpxjRd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51YfMPf0xjRd"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Load CLIP (ViT-B/32)\n",
        "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(model_id); clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "clip_model.to(device); clip_model.eval()"
      ],
      "id": "51YfMPf0xjRd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU8imm1WxjRd",
        "outputId": "95188b04-68ed-4da0-87ae-4f5962895723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#@title Text embeddings from prompts\n",
        "import torch\n",
        "def build_text_matrix(labels, templates):\n",
        "    text_embeds = []\n",
        "    for label in labels:\n",
        "        prompts = [t.format(label=label) for t in templates]\n",
        "        inputs = clip_processor(text=prompts, padding=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            feats = clip_model.get_text_features(**{k:v.to(device) for k,v in inputs.items()})\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        mean_feat = feats.mean(dim=0); mean_feat = mean_feat / mean_feat.norm()\n",
        "        text_embeds.append(mean_feat.cpu())\n",
        "    return torch.stack(text_embeds, dim=0)\n",
        "\n",
        "default_templates = [\"a photo of a {label}\"]\n",
        "text_matrix = build_text_matrix(wanted_classes, default_templates)\n",
        "text_matrix.shape"
      ],
      "id": "IU8imm1WxjRd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8QEV_JexjRe"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Evaluate zero-shot\n",
        "from tqdm import tqdm\n",
        "def image_features(images, batch_size=64):\n",
        "    feats = []\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = images[i:i+batch_size]\n",
        "        inputs = clip_processor(images=batch, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            img_feats = clip_model.get_image_features(**{k:v.to(device) for k,v in inputs.items()})\n",
        "        img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n",
        "        feats.append(img_feats.cpu())\n",
        "    return torch.cat(feats, dim=0)\n",
        "\n",
        "img_feats_test = image_features(X_test_imgs, 64)\n",
        "logits = img_feats_test @ text_matrix.T\n",
        "y_pred = logits.argmax(dim=1).numpy()\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Zero-shot accuracy: {acc:.3f}\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm, display_labels=wanted_classes).plot(values_format='d'); plt.xticks(rotation=45); plt.title(\"Zero-shot CLIP\"); plt.show()"
      ],
      "id": "Y8QEV_JexjRe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-h3nPYWxjRe",
        "outputId": "e0108c01-d0f3-456d-e5c2-05674be1264f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot accuracy with 5 templates: 0.952\n"
          ]
        }
      ],
      "source": [
        "#@title Prompt engineering: multi-template\n",
        "templates = [\"a photo of a {label}\",\n",
        "             \"a blurry photo of a {label}\",\n",
        "             \"a close-up photo of a {label}\",\n",
        "             \"a low-resolution photo of a {label}\",\n",
        "             \"a JPEG photo of a {label}\"]\n",
        "text_matrix = build_text_matrix(wanted_classes, templates)\n",
        "logits = img_feats_test @ text_matrix.T\n",
        "y_pred = logits.argmax(dim=1).numpy()\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Zero-shot accuracy with {len(templates)} templates: {acc:.3f}\")"
      ],
      "id": "7-h3nPYWxjRe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jqfIojExjRe",
        "outputId": "228e482b-118b-49f1-f7d8-bf2a22099e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['<|startoftext|>', 'a</w>', 'close</w>', '-</w>', 'up</w>', 'photo</w>', 'of</w>', 'a</w>', 'cat</w>', '<|endoftext|>']\n",
            "IDs shape: torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "#@title Transformer view: tokenisation\n",
        "prompt = \"a close-up photo of a cat\"\n",
        "enc = tokenizer(prompt, return_tensors=\"pt\")\n",
        "tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
        "print(\"Tokens:\", tokens); print(\"IDs shape:\", enc[\"input_ids\"].shape)"
      ],
      "id": "1jqfIojExjRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPdXci6GxjRf"
      },
      "source": [
        "## 4) Few-shot **linear probe**\n",
        "Train a logistic regression on frozen CLIP features (CPU-friendly)."
      ],
      "id": "EPdXci6GxjRf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQQwBkNpxjRf",
        "outputId": "9358ce82-e4cd-439c-a02f-c7d3ca338825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(750, 512) (500, 512)\n"
          ]
        }
      ],
      "source": [
        "#@title Feature extraction\n",
        "img_feats_train = image_features(X_train_imgs, 64).numpy()\n",
        "img_feats_test_np = img_feats_test.numpy()\n",
        "print(img_feats_train.shape, img_feats_test_np.shape)"
      ],
      "id": "UQQwBkNpxjRf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3Ay8ZpxjRf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Train & compare\n",
        "max_train_per_class = 80\n",
        "counts = [0]*len(wanted_classes); Xtr, ytr = [], []\n",
        "for i,(img,y) in enumerate(zip(X_train_imgs, y_train)):\n",
        "    if counts[y] < max_train_per_class:\n",
        "        Xtr.append(img_feats_train[i]); ytr.append(y); counts[y]+=1\n",
        "    if all(c>=max_train_per_class for c in counts): break\n",
        "Xtr = np.stack(Xtr); ytr = np.array(ytr)\n",
        "clf = LogisticRegression(max_iter=2000, multi_class=\"multinomial\"); clf.fit(Xtr, ytr)\n",
        "y_pred_probe = clf.predict(img_feats_test_np)\n",
        "acc_probe = accuracy_score(y_test, y_pred_probe)\n",
        "print(f\"Linear probe accuracy ({max_train_per_class}/class): {acc_probe:.3f}\")\n",
        "zs_acc = accuracy_score(y_test, (img_feats_test @ text_matrix.T).argmax(dim=1).numpy())\n",
        "plt.figure(figsize=(4,3)); plt.bar([0,1],[zs_acc, acc_probe]); plt.xticks([0,1],[\"Zero-shot\",\"Probe\"]); plt.ylim(0,1); plt.title(\"Accuracy\"); plt.show()"
      ],
      "id": "Dp3Ay8ZpxjRf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA5XYby3xjRf"
      },
      "source": [
        "## 5) Text → Image retrieval\n",
        "Search a small gallery by a text query using CLIP cosine similarity."
      ],
      "id": "eA5XYby3xjRf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X44vK0SaxjRf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Retrieval demo\n",
        "gallery_size = 120; K = 8; query = \"a photo of a small dog\"\n",
        "gallery_imgs = X_test_imgs[:gallery_size]; gallery_feats = img_feats_test[:gallery_size]\n",
        "inputs = clip_processor(text=[query], return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    q = clip_model.get_text_features(**{k:v for k,v in inputs.items()})\n",
        "q = q / q.norm(dim=-1, keepdim=True)\n",
        "sims = (gallery_feats @ q.cpu().T).squeeze(1).numpy()\n",
        "topk_idx = np.argsort(-sims)[:K]\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, idx in enumerate(topk_idx):\n",
        "    plt.subplot(1,K,i+1); plt.imshow(gallery_imgs[idx]); plt.axis('off'); plt.title(f\"#{i+1}\")\n",
        "plt.suptitle(f\"Query: {query}\"); plt.show()\n",
        "# added to Gitbub"
      ],
      "id": "X44vK0SaxjRf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYFxNz6FxjRg"
      },
      "source": [
        "## 6) Extend into your coursework\n",
        "**Swap dataset**, design **prompt families**, run **ablation** on shots, add **calibration**, discuss **risks/ethics**, and propose an **ops plan** (cost, deployment, monitoring). Keep everything reproducible."
      ],
      "id": "dYFxNz6FxjRg"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}